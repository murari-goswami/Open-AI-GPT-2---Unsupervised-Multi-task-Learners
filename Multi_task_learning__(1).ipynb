{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Multi_task_learning_ (1).ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/soumyadip1995/Open-AI-GPT-2---Unsupervised-Multi-task-Learners/blob/master/Multi_task_learning__(1).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "KHEaeqXHgggn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Summary\n",
        "\n",
        "Lets think about how a Recurrent Neural Network works. RNNs can be used to predict the next word as well , they can also be used to predict a word after a long sequence of text in language models , they are used in machine Translation and Generative models. But why is it that RNNs are not considered to be dangerous but the Open AI's GPT-2 model is being considered dangerous, so much so that OpenAI felt that releasing the full version would be dangerous. Instead they released a much smaller version. The reason why this is being said is because the GPT-2 model is based on the notion that language models are unsupervised Multi Task Learners. This is actually the heading of the original paper by the way. \n",
        ".\n",
        "MLT is basically a way of utilising additional featuresand related tasks in a neural network which improves generalization of a NN. Now, imagine if this was unsupervised and the entire thing was performed inside a zero shot setting , therefore increasing the accuracy of the language model . Read the full blog below to see why is this so important.\n",
        "\n",
        "\n",
        "In this part we will be Learning about a few Things:-\n",
        " 1. Multi Task Learning\n",
        " 2.  The OpenAI GPT-2\n",
        " 3. Transformer Model\n",
        " 4. Zero Shot Learning\n",
        " 5. Connecting The two approaches."
      ]
    },
    {
      "metadata": {
        "id": "NyLBkX5MBTy7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Introduction to Multi Task Learning\n",
        "\n",
        "In Machine Learning (ML), we typically care about optimizing for a particular metric, whether this is a score on a certain benchmark or a business KPI. In order to do this, we generally train a single model or an ensemble of models to perform our desired task. We then fine-tune and tweak these models until their performance no longer increases. While we can generally achieve acceptable performance this way, by being laser-focused on our single task, we ignore information that might help us do even better on the metric we care about. Specifically, this information comes from the training signals of related tasks. By sharing representations between related tasks, we can enable our model to generalize better on our original task.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "##Motivation\n",
        "\n",
        "We can motivate multi-task learning in different ways: Biologically, we can see multi-task learning as being inspired by human learning. For learning new tasks, we often apply the knowledge we have acquired by learning related tasks. For instance, a baby first learns to recognize faces and can then apply this knowledge to recognize other objects.\n",
        "\n",
        "We can view multi-task learning as a form of inductive transfer. Inductive transfer can help improve a model by introducing an inductive bias, which causes a model to prefer some hypotheses over others. For instance, a common form of inductive bias is:- \n",
        "**ℓ1** regularization, which leads to a preference for sparse solutions. In the case of MTL, the  ***inductive bias*** is provided by the **auxiliary tasks**, which cause the model to prefer hypotheses that explain more than one task. As we will see shortly, this generally leads to solutions that generalize better.\n",
        "\n",
        "\n",
        "###Auxiliary tasks\n",
        "\n",
        "MTL is a natural fit in situations where we are interested in obtaining predictions for multiple tasks at once. Such scenarios are common for instance in finance or economics forecasting, where we might want to predict the value of many possibly related indicators, or in bioinformatics where we might want to predict symptoms for multiple diseases simultaneously. In scenarios such as drug discovery, where tens or hundreds of active compounds should be predicted, MTL accuracy increases continuously with the number of tasks .\n",
        "\n",
        "In most situations, however, we only care about performance on one task. In this section, we will thus look at how we can find a suitable auxiliary task in order to still reap the benefits of multi-task learning."
      ]
    },
    {
      "metadata": {
        "id": "0g8e3bBpFCSA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Two MTL methods for Deep Learning\n",
        "\n",
        "So far, we have focused on theoretical motivations for MTL. To make the ideas of MTL more concrete, we will now look at the two most commonly used ways to perform multi-task learning in deep neural networks. In the context of Deep Learning, multi-task learning is typically done with either hard or soft parameter sharing of hidden layers.\n",
        "\n",
        "\n",
        "###1. Hard parameter sharing\n",
        "\n",
        "\n",
        "Hard parameter sharing is the most commonly used approach to MTL in neural networks. It is generally applied by sharing the hidden layers between all tasks, while keeping several task-specific output layers. Hard parameter sharing greatly reduces the risk of overfitting(ruder.io)\n",
        "\n",
        "![alt text](http:/ruder.io/content/images/2017/05/mtl_images-001-2.png) "
      ]
    },
    {
      "metadata": {
        "id": "jvpYvAN3MgQ3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###2. Soft parameter sharing\n",
        "In soft parameter sharing on the other hand, each task has its own model with its own parameters. The distance between the parameters of the model is then regularized in order to encourage the parameters to be similar.for instance use the \n",
        "***ℓ2*** norm for regularization, while use the trace norm.(ruder.io)\n",
        "\n",
        "![alt text](http://ruder.io/content/images/2017/05/mtl_images-002-1.png)\n",
        "\n",
        "\n",
        "##Why does MTL work?\n",
        "\n",
        "Even though an inductive bias obtained through multi-task learning seems intuitively plausible, in order to understand MTL better, we need to look at the mechanisms that underlie it. Most of these have first been proposed by Caruana (1998). For all examples, we will assume that we have two related tasks \n",
        "A\n",
        " and \n",
        "B\n",
        ", which rely on a common hidden layer representation \n",
        "F\n",
        ".\n",
        "\n",
        "##Implicit data augmentation\n",
        "MTL effectively increases the sample size that we are using for training our model. As all tasks are at least somewhat noisy, when training a model on some task \n",
        "A\n",
        ", our aim is to learn a good representation for task \n",
        "A\n",
        " that ideally ignores the data-dependent noise and generalizes well. As different tasks have different noise patterns, a model that learns two tasks simultaneously is able to learn a more general representation. Learning just task \n",
        "A\n",
        " bears the risk of overfitting to task \n",
        "A\n",
        ", while learning \n",
        "A\n",
        " and \n",
        "B\n",
        "jointly enables the model to obtain a better representation \n",
        "F\n",
        " through averaging the noise patterns.\n",
        "\n",
        "##Attention focusing\n",
        "If a task is very noisy or data is limited and high-dimensional, it can be difficult for a model to differentiate between relevant and irrelevant features. MTL can help the model focus its attention on those features that actually matter as other tasks will provide additional evidence for the relevance or irrelevance of those features."
      ]
    },
    {
      "metadata": {
        "id": "eFTcZ0H1NoQr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Regularization\n",
        "\n",
        "Finally, MTL acts as a regularizer by introducing an inductive bias. As such, it reduces the risk of overfitting and has the ability of fitting the random noise as well."
      ]
    },
    {
      "metadata": {
        "id": "G5zGtE9PZB46",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Open AI GPT-2\n",
        "\n",
        "##Summary\n",
        "\n",
        "Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on task specific datasets. The new model called GPT-2 , was trained simply to predict the next word in 40GB of Internet text. GPT-2 is a large transformer-based language model with 1.5 billion parameters, trained on a dataset of 8 million web pages. GPT-2 is trained with a simple objective: predict the next word, given all of the previous words within some text. The diversity of the dataset causes this simple goal to contain naturally occurring demonstrations of many tasks across diverse domains. GPT-2 is a direct scale-up of GPT, with more than 10x the parameters and trained on more than 10x the amount of data.\n",
        "\n",
        "\n",
        "\n",
        "GPT-2 displays a broad set of capabilities, including the ability to generate conditional synthetic text samples of unprecedented quality, where we prime the model with an input and have it generate a lengthy continuation. In addition, GPT-2 outperforms other language models trained on specific domains (like Wikipedia, news, or books) without needing to use these domain-specific training datasets. On language tasks like question answering, reading comprehension, summarization, and translation, GPT-2 begins to learn these tasks from the raw text, using no task-specific training data. \n",
        "\n",
        "\n",
        "##Transformer model\n",
        "\n",
        "Most competitive neural sequence transduction models have an encoder-decoder structure. Below is how sequence transduction works.\n",
        "\n",
        "![alt text](https://3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com/wp-content/uploads/2017/08/Transducer-RNN-Training-Graph.png)\n",
        "\n",
        "\n",
        "\n",
        "Here, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence\n",
        "of continuous representations s = (s1, ..., sn). Given z, the decoder then generates an output\n",
        "sequence (y1, ..., ym) of symbols one element at a time. The Transformer follows this overall architecture using stacked self-attention and point-wise, fully\n",
        "connected layers for both the encoder and decoder, shown in the left and right halves of the Figure ,\n",
        "respectively.(From GPT paper). Below is the Full Transformer Model.\n",
        "\n",
        "![alt text](https://cdn-images-1.medium.com/max/1600/1*BHzGVskWGS_3jEcYYi6miQ.png)\n",
        "\n",
        "\n",
        "The capacity of the language model is essential\n",
        "to the success of zero-shot task transfer and increasing it improves performance in a log-linear\n",
        "fashion across tasks.\n",
        "\n",
        "##Zero Shot Learning\n",
        "\n",
        "Zero-shot learning is being able to solve a task despite not having received any training examples of that task. For a concrete example, imagine recognizing a category of object in photos without ever having seen a photo of that kind of object before. If you've read a very detailed description of a cat, you might be able to tell what a cat is in a photograph the first time you see it.\n",
        "\n",
        "\n",
        "\n",
        "##Unsupervised multi task learning \n",
        "\n",
        "Multitask learning is therefore a promising framework for improving general performance. However, multitask training in NLP is still nascent. Recent work reports modest performance improvements and the two most ambitious efforts to date have\n",
        "trained on a total of 10 and 17 (dataset, objective)\n",
        "pairs respectively . From a meta-learning perspective, each (dataset,\n",
        "objective) pair is a single training example sampled\n",
        "from the distribution of datasets and objectives. Current\n",
        "ML systems need hundreds to thousands of examples to\n",
        "induce functions which generalize well. This suggests that\n",
        "multitask training many need just as many effective training\n",
        "pairs to realize its promise with current approaches. It will\n",
        "be very difficult to continue to scale the creation of datasets\n",
        "and the design of objectives to the degree that may be required to brute force our way there with current techniques.\n",
        "This motivates exploring additional setups for performing\n",
        "multitask learning. This opens up another realm of work, to perform unsupervised MLT"
      ]
    },
    {
      "metadata": {
        "id": "ImyDA_Shdb1e",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Approach\n",
        "### We connect these two lines of work i.e; **multi task Learning** and **Zero Shot Learning** and continue the trend of more general methods of transfer. \n",
        "\n",
        "We\n",
        "demonstrate language models can perform down-stream\n",
        "tasks in a zero-shot setting – without any parameter or architecture modification. \n",
        "\n",
        "###We demonstrate this approach shows potential by highlighting the ability of language models perform a wide range of tasks in a zero-shot setting .\n",
        "\n",
        "We\n",
        "achieve promising, competitive, and state of the art results\n",
        "depending on the task.\n",
        "\n",
        "\n",
        "At the core of the approach is language modeling. Language modeling is usually framed as unsupervised distribution estimation from a set of examples (x1, x2, ..., xn)\n",
        "each composed of variable length sequences of symbols\n",
        "(s1, s2, ..., sn). Since language has a natural sequential ordering, it is common to factorize the joint probabilities over\n",
        "symbols as the product of conditional probabilities \n",
        "\n",
        "This approach allows for tractable sampling from and estimation of p(x) as well as any conditionals of the form\n",
        "\n",
        "$$p(s_{n−k}... s_n|s_1... s_{n−k−1})$$\n",
        "\n",
        "In recent years, there have\n",
        "been significant improvements in the expressiveness of models that can compute these conditional probabilities, such as\n",
        "self-attention architectures like the Transformer. Shown Above.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "$$\\begin{equation*}\n",
        "p(x) =\n",
        "\\prod_{i=1}^{n}\\ p(s_n|s_1 ... s_n−1)\n",
        "\\quad\\quad \\text{}.\n",
        "\\end{equation*}$$\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "o1ossP_xiAM6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "![alt text](https://venturebeat.com/wp-content/uploads/2019/02/nlp_openai_trainingPNG.png?w=565&resize=565%2C456&strip=all)\n",
        "\n",
        "GPT-2 achieves state-of-the-art on Winograd Schema, LAMBADA, and other language modeling tasks.\n"
      ]
    },
    {
      "metadata": {
        "id": "okyxEDlhzvmN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Code for Multi Task Learning"
      ]
    },
    {
      "metadata": {
        "id": "B4tPhAHzy29c",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "import matplotlib.pylab as plt\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "from keras.models import Model\n",
        "from keras.layers import Input\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import MaxPooling2D, GlobalMaxPooling2D\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "import tensorflow as tf\n",
        "from keras import backend as K\n",
        "from keras import initializers\n",
        "\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "\n",
        "\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "tf.set_random_seed(42)\n",
        "my_init = initializers.glorot_uniform(seed=42)\n",
        "\n",
        "\n",
        "session_conf = tf.ConfigProto(intra_op_parallelism_threads=1,\n",
        "                              inter_op_parallelism_threads=1)\n",
        "sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
        "K.set_session(sess)\n",
        "\n",
        "\n",
        "X, Y1, Y2, Y3 = make_x_y(points, images, labels, labels2)\n",
        "\n",
        "X_train, X_test, Y_train, Y_test, Y_train2, Y_test2, Y_train3, Y_test3 = prepare_train_test(X, Y1, Y2, Y3, W)\n",
        "# 1: emotions\n",
        "# 2: FACS\n",
        "# 3: facial keypoints\n",
        "\n",
        "\n",
        "input_shape = (W, W, 1)\n",
        "visible = Input(shape=input_shape)\n",
        "x = Conv2D(10, kernel_size=3, activation='elu', padding='same', kernel_initializer=my_init)(visible)\n",
        "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
        "x = Conv2D(20, kernel_size=3, activation='elu', padding='same', kernel_initializer=my_init)(x)\n",
        "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
        "x = Conv2D(30, kernel_size=3, activation='elu', padding='same', kernel_initializer=my_init)(x)\n",
        "x = GlobalMaxPooling2D()(x)\n",
        "output = Dense(Y_train.shape[1], activation='softmax', kernel_initializer=my_init)(x)\n",
        "model = Model(inputs=visible, outputs=output)\n",
        "\n",
        "\n",
        "\n",
        "model.summary()\n",
        "\n",
        "model.compile(loss=['categorical_crossentropy'],\n",
        "              optimizer=Adam(clipnorm = 1.), metrics = ['accuracy'])\n",
        "\n",
        "\n",
        "model.fit(X_train, Y_train,\n",
        "          validation_data = (X_test, Y_test),\n",
        "          batch_size = 64,\n",
        "          epochs = 10,\n",
        "          verbose = True,\n",
        "          shuffle = False)\n",
        "\n",
        "\n",
        "pred = model.predict(X_test)\n",
        "y_pred = [np.argmax(p) for p in pred]\n",
        "y_true = [np.argmax(p) for p in Y_test]\n",
        "\n",
        "print classification_report(y_true, y_pred)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IvmBA-c03YlU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Conclusion\n",
        " \n",
        " So, Ultimately it is unsupervised. We are performing unsupervised multi task learning in a zero shot setting. That is one of the more efficient ways. We take any dataset as per the picture above (say LAMBADA) and we are putting it in a **zero shot setting **where unsupervised MLT is being performed. In a zero shot setting, the data enters a **sequence transduction model**(Transformer model) where the output probabilities are received. Those output Probabilities within the zero shot setting are sampled(**Neural Sequence Transduction** ) and unsupervised MLT is performed on that, the complete Learning is done on additional features, therefore enhancing the capacity of the language model. So, essentially no training is being performed here.\n",
        " \n",
        " \n",
        " Results suggest that unsupervised task learning is an\n",
        "additional promising area of research to explore. These\n",
        "findings potentially help explain the widespread success of\n",
        "pre-training techniques for NLP tasks as we\n",
        "show that, in the limit, one of these pre-training techniques\n",
        "begins to learn to perform tasks directly without the need\n",
        "for supervised adaption or modification.\n",
        "\n",
        "Large, general language models could have significant societal impacts, and also have many near-term applications. We can anticipate how systems like GPT-2 could be used to create:\n",
        "\n",
        "1. AI writing assistants\n",
        "2. More capable dialogue agents\n",
        "3. Unsupervised translation between languages\n",
        "4. Better speech recognition systems\n",
        "\n",
        "\n",
        "These findings, combined with earlier results on synthetic imagery, audio, and video, imply that technologies are reducing the cost of generating fake content and waging disinformation campaigns. The public at large will need to become more skeptical of text they find online, just as the ”deep fakes\" phenomenon calls for more skepticism about images. (Issued by OpenAI). So, you can see how these can be detrimental."
      ]
    }
  ]
}